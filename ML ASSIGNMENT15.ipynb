{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "045daaa3",
   "metadata": {},
   "source": [
    "1. Recognize the differences between supervised, semi-supervised, and unsupervised learning.\n",
    "2. Describe in detail any five examples of classification problems.\n",
    "3. Describe each phase of the classification process in detail.\n",
    "\n",
    "4. Go through the SVM model in depth using various scenarios.\n",
    "\n",
    "5. What are some of the benefits and drawbacks of SVM?\n",
    "\n",
    "6. Go over the kNN model in depth.\n",
    "\n",
    "7. Discuss the kNN algorithm&#39;s error rate and validation error.\n",
    "\n",
    "8. For kNN, talk about how to measure the difference between the test and training results.\n",
    "\n",
    "9. Create the kNN algorithm.\n",
    "\n",
    "10. What is a decision tree, exactly? What are the various kinds of nodes? Explain all in depth.\n",
    "\n",
    "11. Describe the different ways to scan a decision tree.\n",
    "\n",
    "12. Describe in depth the decision tree algorithm.\n",
    "\n",
    "13. In a decision tree, what is inductive bias? What would you do to stop overfitting?\n",
    "\n",
    "14. Explain advantages and disadvantages of using a decision tree?\n",
    "\n",
    "15. Describe in depth the problems that are suitable for decision tree learning.\n",
    "\n",
    "16. Describe in depth the random forest model. What distinguishes a random forest?\n",
    "\n",
    "17. In a random forest, talk about OOB error and variable value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb1efb7",
   "metadata": {},
   "source": [
    "Ans 1:\n",
    "\n",
    "Supervised Learning: In supervised learning, the algorithm is trained on a labeled dataset, meaning each example in the dataset is paired with the correct output. The goal is to learn a mapping from inputs to outputs.\n",
    "\n",
    "Semi-Supervised Learning: Semi-supervised learning combines both labeled and unlabeled data for training. Typically, there is a large amount of unlabeled data and a smaller set of labeled data. The algorithm uses the unlabeled data to improve performance by leveraging the underlying structure in the data.\n",
    "\n",
    "Unsupervised Learning: Unsupervised learning involves training algorithms with data that doesn't have explicit labels. The goal is to find inherent patterns or relationships in the data, such as grouping or clustering.\n",
    "\n",
    "Ans 2:\n",
    "Five examples of classification problems:\n",
    "\n",
    "Email Spam Detection: Classifying emails as either spam or non-spam.\n",
    "Sentiment Analysis: Classifying text as expressing positive, negative, or neutral sentiment.\n",
    "Medical Diagnosis: Classifying patients as having a particular disease or not based on symptoms and test results.\n",
    "Handwritten Digit Recognition: Classifying handwritten digits into their respective numbers.\n",
    "Credit Card Fraud Detection: Classifying credit card transactions as either genuine or fraudulent.\n",
    "\n",
    "Ans 3:\n",
    "The classification process typically involves the following phases:\n",
    "\n",
    "Data Collection: Gathering labeled training data.\n",
    "Data Preprocessing: Cleaning and preparing the data for training.\n",
    "Feature Selection/Extraction: Identifying relevant features from the data.\n",
    "Model Training: Training the classification model on the labeled data.\n",
    "Evaluation: Assessing the model's performance on a separate test dataset.\n",
    "Deployment: Deploying the trained model for making predictions on new data.\n",
    "\n",
    "Ans 4:\n",
    "Support Vector Machine (SVM) is a powerful supervised learning algorithm used for classification and regression tasks. It works by finding the hyperplane that best separates different classes in the feature space. In various scenarios:\n",
    "\n",
    "For linearly separable data, SVM aims to find the hyperplane with the maximum margin between classes.\n",
    "For non-linearly separable data, SVM uses the kernel trick to map the data into a higher-dimensional space where it becomes linearly separable.\n",
    "\n",
    "Ans 5:\n",
    "Benefits of SVM:\n",
    "\n",
    "Effective in high-dimensional spaces.\n",
    "Versatile due to the kernel trick for non-linear data.\n",
    "Drawbacks of SVM:\n",
    "\n",
    "Computationally intensive, especially with large datasets.\n",
    "Can be sensitive to the choice of kernel and regularization parameters.\n",
    "\n",
    "Ans 6:\n",
    "k-Nearest Neighbors (kNN) is a simple yet effective supervised learning algorithm used for classification and regression tasks. It classifies a data point based on how its neighbors are classified.\n",
    "\n",
    "Ans 7:\n",
    "The kNN algorithm's error rate and validation error are typically evaluated using techniques like cross-validation. The error rate is the proportion of incorrect predictions, while the validation error is the error rate on a validation dataset, which is used for tuning hyperparameters.\n",
    "\n",
    "Ans 8:\n",
    "The difference between test and training results in kNN can be measured using metrics like accuracy, precision, recall, or the F1-score. These metrics quantify how well the algorithm's predictions match the actual labels in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0c831a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ans:9\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def euclidean_distance(x1, x2):\n",
    "    return np.sqrt(np.sum((x1 - x2)**2))\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=3):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = [self._predict(x) for x in X]\n",
    "        return np.array(y_pred)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "        k_indices = np.argsort(distances)[:self.k]\n",
    "        k_nearest_labels = [self.y_train[i] for i in k_indices]\n",
    "        most_common = Counter(k_nearest_labels).most_common(1)\n",
    "        return most_common[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd401f",
   "metadata": {},
   "source": [
    "Ans 10:\n",
    "A decision tree is a supervised learning algorithm that learns a hierarchical structure by recursively splitting the data based on feature conditions. The tree consists of:\n",
    "\n",
    "Root Node: Represents the entire dataset.\n",
    "Internal Nodes: Represent feature conditions for data splitting.\n",
    "Leaf Nodes: Represent the final decision or prediction.\n",
    "\n",
    "Ans 11:\n",
    "Ways to scan a decision tree include:\n",
    "\n",
    "Depth-First Search (DFS): Traverse down the tree until reaching a leaf node, then backtrack.\n",
    "Breadth-First Search (BFS): Explore all nodes at the current depth before moving to the next depth.\n",
    "\n",
    "Ans 12:\n",
    "The decision tree algorithm recursively splits the data based on feature conditions to create a tree structure that best separates the classes or predicts the target variable. It uses criteria like Gini impurity or entropy to decide the best feature and condition for splitting at each node.\n",
    "\n",
    "Ans 13:\n",
    "Inductive bias in a decision tree refers to the assumptions or biases inherent in the learning algorithm that guide it to prefer certain hypotheses over others. To prevent overfitting in decision trees, techniques like pruning, setting a maximum depth, or using ensemble methods can be employed.\n",
    "\n",
    "Ans 14:\n",
    "Advantages of Decision Trees:\n",
    "\n",
    "Easy to interpret and visualize.\n",
    "Can handle both numerical and categorical data.\n",
    "Disadvantages of Decision Trees:\n",
    "\n",
    "Prone to overfitting, especially with deep trees.\n",
    "Can be sensitive to small changes in data.\n",
    "\n",
    "Ans 15:\n",
    "Decision tree learning is suitable for problems with:\n",
    "\n",
    "Categorical or numerical target variables.\n",
    "Complex interactions between features.\n",
    "Problems where interpretability is important.\n",
    "\n",
    "Ans 16:\n",
    "Random Forest is an ensemble learning method that combines multiple decision trees to improve performance and reduce overfitting. What distinguishes a random forest is:\n",
    "\n",
    "Bagging: Each tree is trained on a bootstrap sample of the data.\n",
    "Feature Randomness: At each split, a random subset of features is considered.\n",
    "\n",
    "Ans 17:\n",
    "In a random forest:\n",
    "\n",
    "OOB (Out-of-Bag) Error: It estimates the performance of the model on unseen data using the samples not included in a particular bootstrap sample.\n",
    "Variable Importance: Random forests provide a measure of feature importance based on how much each feature reduces the impurity in the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5293b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
