{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae58e7b9",
   "metadata": {},
   "source": [
    "1. What are Sequence-to-sequence models?\n",
    "2. What are the Problem with Vanilla RNNs?\n",
    "3. What is Gradient clipping?\n",
    "4. Explain Attention mechanism\n",
    "5. Explain Conditional random fields (CRFs)\n",
    "6. Explain self-attention\n",
    "7. What is Bahdanau Attention?\n",
    "8. What is a Language Model?\n",
    "9. What is Multi-Head Attention?\n",
    "10. What is Bilingual Evaluation Understudy (BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f3004d",
   "metadata": {},
   "source": [
    "Ans 1: Sequence-to-Sequence Models\n",
    "\n",
    "Sequence-to-sequence models, often called Seq2Seq models, are neural network architectures used primarily for tasks that involve generating a sequence output from a sequence input. These models consist of two main components: an encoder, which processes the input sequence, and a decoder, which generates the output sequence. They have been successfully applied to various tasks such as machine translation, text summarization, and speech recognition.\n",
    "\n",
    "Ans 2: Problems with Vanilla RNNs\n",
    "\n",
    "Vanishing and Exploding Gradients: Vanilla RNNs suffer from the vanishing and exploding gradients problem, where gradients either become too small to affect the network's weights or too large, causing instability during training.\n",
    "\n",
    "Short-Term Memory: Vanilla RNNs struggle to capture long-range dependencies in the input sequences due to their limited memory capacity.\n",
    "\n",
    "Inefficient for Long Sequences: As sequences grow longer, the computational complexity and memory requirements of vanilla RNNs increase significantly.\n",
    "\n",
    "Ans 3: Gradient Clipping\n",
    "\n",
    "Gradient clipping is a technique used to mitigate the exploding gradients problem in neural networks. It involves scaling the gradients when their norm exceeds a specified threshold. By limiting the gradient's magnitude, gradient clipping helps stabilize the training process, making it less prone to divergence and improving the model's convergence.\n",
    "\n",
    "Ans 4: Attention Mechanism\n",
    "\n",
    "Attention mechanisms allow neural networks to focus on different parts of the input when producing an output. Unlike traditional Seq2Seq models, where the entire input sequence is encoded into a fixed-size context vector, attention-based models generate context vectors dynamically, weighting the input sequence's elements based on their relevance to the current decoding step. This enables the model to capture complex patterns and dependencies in the input data more effectively.\n",
    "\n",
    "Ans 5: Conditional Random Fields (CRFs)\n",
    "\n",
    "Conditional Random Fields (CRFs) are a type of probabilistic graphical model used for structured prediction tasks, such as sequence labeling and segmentation. Unlike traditional models that make independent predictions for each input element, CRFs model the dependencies between neighboring elements in the output sequence, allowing them to capture contextual information and produce more coherent predictions.\n",
    "\n",
    "Ans 6: Self-Attention\n",
    "\n",
    "Self-attention, also known as intra-attention, is an attention mechanism that computes the attention weights based solely on the input sequence itself, without relying on external context or additional information. It allows the model to weigh the importance of different input elements relative to each other, enabling it to capture long-range dependencies and relationships within the sequence more effectively.\n",
    "\n",
    "Ans 7: Bahdanau Attention\n",
    "\n",
    "Bahdanau Attention, named after its creator Dzmitry Bahdanau, is a type of attention mechanism commonly used in Seq2Seq models for machine translation and other sequence-to-sequence tasks. Unlike traditional attention mechanisms that generate context vectors based on fixed alignments between the input and output sequences, Bahdanau Attention uses learned alignments, allowing the model to adaptively focus on different parts of the input sequence at each decoding step.\n",
    "\n",
    "Ans 8: Language Model\n",
    "\n",
    "A language model is a probabilistic model that predicts the likelihood of a sequence of words or characters appearing in a given language. It captures the statistical properties of the language, such as word frequencies and syntactic patterns, and can be used for various natural language processing tasks, including text generation, speech recognition, and machine translation.\n",
    "\n",
    "Ans 9: Multi-Head Attention\n",
    "\n",
    "Multi-Head Attention is an extension of the standard attention mechanism that uses multiple sets of attention weights to generate context vectors from the input sequence. By allowing the model to attend to different parts of the input in parallel, Multi-Head Attention can capture a broader range of dependencies and relationships, leading to improved performance on tasks that require understanding complex sequences.\n",
    "\n",
    "Ans 10: Bilingual Evaluation Understudy (BLEU)\n",
    "\n",
    "Bilingual Evaluation Understudy (BLEU) is a metric used to evaluate the quality of machine-translated text by comparing it to one or more reference translations. It measures the n-gram overlap between the generated and reference translations, giving higher scores to translations that contain more overlapping n-grams. BLEU is widely used in machine translation research and has become a standard benchmark for evaluating the performance of translation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a62b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
