{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "385ae99b",
   "metadata": {},
   "source": [
    "1. What are Vanilla autoencoders\n",
    "2. What are Sparse autoencoders\n",
    "3. What are Denoising autoencoders\n",
    "4. What are Convolutional autoencoders\n",
    "5. What are Stacked autoencoders\n",
    "6. Explain how to generate sentences using LSTM autoencoders\n",
    "7. Explain Extractive summarization\n",
    "8. Explain Abstractive summarization\n",
    "9. Explain Beam search\n",
    "10. Explain Length normalization\n",
    "11. Explain Coverage normalization\n",
    "12. Explain ROUGE metric evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80e1e9a",
   "metadata": {},
   "source": [
    "Ans 1: Vanilla Autoencoders\n",
    "\n",
    "Vanilla autoencoders, also known as standard autoencoders, are neural network architectures designed for unsupervised learning tasks. They consist of an encoder and a decoder, with the encoder compressing the input data into a lower-dimensional representation (latent space), and the decoder reconstructing the original data from this representation. The goal of vanilla autoencoders is to minimize the reconstruction error, ensuring that the output closely matches the input.\n",
    "\n",
    "Ans 2: Sparse Autoencoders\n",
    "\n",
    "Sparse autoencoders are a variant of autoencoders that incorporate sparsity constraints into their training process. By encouraging the activation of only a subset of neurons in the hidden layers, sparse autoencoders aim to learn more meaningful and interpretable representations of the input data. This can help improve the model's ability to generalize to new, unseen data and may also lead to more efficient representations.\n",
    "\n",
    "Ans 3: Denoising Autoencoders\n",
    "\n",
    "Denoising autoencoders are designed to learn robust representations by reconstructing the original data from corrupted versions of it. During training, noise (e.g., random dropout or Gaussian noise) is added to the input data, and the model is trained to recover the clean, uncorrupted data. By learning to ignore the noise and focus on the underlying patterns in the data, denoising autoencoders can improve the quality of the learned representations and make the model more resilient to noisy input.\n",
    "\n",
    "Ans 4: Convolutional Autoencoders\n",
    "\n",
    "Convolutional autoencoders are a type of autoencoder architecture that utilizes convolutional layers to process and reconstruct spatial data, such as images. By leveraging the spatial hierarchies and local patterns present in the input data, convolutional autoencoders can capture complex features and structures more effectively than traditional fully connected autoencoders, making them well-suited for tasks like image denoising, reconstruction, and generation.\n",
    "\n",
    "Ans 5: Stacked Autoencoders\n",
    "\n",
    "Stacked autoencoders, also known as deep autoencoders, are composed of multiple layers of encoders and decoders stacked on top of each other. Each layer learns a progressively more abstract representation of the input data, with the final layer encoding the data into a compact latent representation. By training the entire network jointly, stacked autoencoders can learn complex hierarchical representations and capture intricate patterns in the data.\n",
    "\n",
    "Ans 6: Generating Sentences using LSTM Autoencoders\n",
    "\n",
    "To generate sentences using LSTM (Long Short-Term Memory) autoencoders, the model is trained to encode input sentences into a fixed-length latent representation using an LSTM encoder. During the decoding phase, the latent representation is fed into an LSTM decoder, which generates words or tokens one at a time until a complete sentence is produced. By learning to reconstruct the original input sentences, LSTM autoencoders can capture the underlying structure and semantics of the language, enabling them to generate coherent and contextually relevant sentences.\n",
    "\n",
    "Ans 7: Extractive Summarization\n",
    "\n",
    "Extractive summarization is a text summarization technique that involves selecting and extracting important sentences or passages from the original text to create a concise summary. Unlike abstractive summarization, which generates new sentences to capture the main ideas and concepts, extractive summarization relies on identifying and extracting existing content from the input text. This approach is often simpler and more straightforward but may not capture the full context or produce as fluent summaries as abstractive methods.\n",
    "\n",
    "Ans 8: Abstractive Summarization\n",
    "\n",
    "Abstractive summarization is a text summarization technique that generates a concise summary by synthesizing new sentences or phrases that capture the main ideas and concepts present in the original text. Unlike extractive summarization, which simply selects and rearranges existing content, abstractive summarization requires the model to understand and interpret the input text, generate coherent and grammatically correct sentences, and produce a summary that captures the essential information and context.\n",
    "\n",
    "Ans 9: Beam Search\n",
    "\n",
    "Beam search is a heuristic search algorithm commonly used in sequence generation tasks, such as machine translation and text generation. Instead of exhaustively exploring all possible sequences, beam search maintains a fixed-size set of candidate sequences (known as the beam) and expands it iteratively by considering the most promising options at each step. By keeping track of multiple candidate sequences and exploring them in parallel, beam search can generate more coherent and contextually relevant outputs than simpler search strategies.\n",
    "\n",
    "Ans 10: Length Normalization\n",
    "\n",
    "Length normalization is a technique used to adjust the scores or probabilities generated by sequence models, such as neural machine translation models, to account for the length of the generated sequences. By dividing the model's output by the length of the sequence raised to a certain power (e.g., 0.6), length normalization helps mitigate the length bias issue, where longer sequences tend to have lower probabilities or scores compared to shorter ones, and encourages the model to produce more diverse and varied outputs.\n",
    "\n",
    "Ans 11: Coverage Normalization\n",
    "\n",
    "Coverage normalization is a technique used to address the problem of repeating or skipping words/phrases in sequence generation tasks, such as neural machine translation. By maintaining a coverage vector that keeps track of the attention scores assigned to different input positions, coverage normalization penalizes the model for repeatedly attending to the same positions and encourages it to cover all relevant parts of the input sequence, leading to more coherent and accurate translations.\n",
    "\n",
    "Ans 12: ROUGE Metric Evaluation\n",
    "\n",
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics used to evaluate the quality of text summarization and machine translation outputs by comparing them to one or more reference summaries or translations. It measures various aspects such as n-gram overlap, sentence-level similarity, and word order to assess the overall similarity and content overlap between the generated and reference texts. ROUGE scores are widely used in research and development to benchmark and compare the performance of different summarization and translation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641aa5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
