{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b3fa07",
   "metadata": {},
   "source": [
    "1. What is feature engineering, and how does it work? Explain the various aspects of feature\n",
    "engineering in depth.\n",
    "2. What is feature selection, and how does it work? What is the aim of it? What are the various\n",
    "methods of function selection?\n",
    "3. Describe the function selection filter and wrapper approaches. State the pros and cons of each\n",
    "approach?\n",
    "\n",
    "4.\n",
    "\n",
    "i. Describe the overall feature selection process.\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most\n",
    "widely used function extraction algorithms?\n",
    "\n",
    "5. Describe the feature engineering process in the sense of a text categorization issue.\n",
    "\n",
    "6. What makes cosine similarity a good metric for text categorization? A document-term matrix has\n",
    "two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in\n",
    "cosine.\n",
    "\n",
    "7.\n",
    "\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
    "calculate the Hamming gap.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
    "0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
    "\n",
    "8. State what is meant by &quot;high-dimensional data set&quot;? Could you offer a few real-life examples?\n",
    "What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
    "What can be done about it?\n",
    "\n",
    "9. Make a few quick notes on:\n",
    "\n",
    "a.PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "b. Use of vectors\n",
    "\n",
    "c. Embedded technique\n",
    "\n",
    "10. Make a comparison between:\n",
    "\n",
    "a. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "b. Function selection methods: filter vs. wrapper\n",
    "\n",
    "c. SMC vs. Jaccard coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbe9756",
   "metadata": {},
   "source": [
    "Ans 1:\n",
    "Feature engineering is the process of creating new features or transforming existing ones to improve a model's performance or interpretability. It involves:\n",
    "\n",
    "Feature Creation: Generating new features from existing ones, e.g., creating polynomial features.\n",
    "Feature Transformation: Modifying existing features to meet model assumptions, e.g., log transformation.\n",
    "Feature Scaling: Standardizing or normalizing features to a consistent scale.\n",
    "Feature Encoding: Converting categorical variables into numerical format using techniques like one-hot encoding.\n",
    "\n",
    "Ans 2:\n",
    "Feature selection involves choosing the most relevant features to include in a model, aiming to improve performance, reduce overfitting, and enhance interpretability. Methods include:\n",
    "\n",
    "Filter Methods: Evaluating features based on statistical measures like correlation, chi-squared tests.\n",
    "Wrapper Methods: Evaluating feature subsets using specific models and performance metrics, e.g., recursive feature elimination.\n",
    "Embedded Methods: Incorporating feature selection within the model training process, e.g., Lasso regression.\n",
    "\n",
    "Ans 3:\n",
    "\n",
    "Filter Approach: Evaluates features independently of any model, using statistical measures or other criteria.\n",
    "Pros: Computationally efficient, less prone to overfitting.\n",
    "Cons: Might overlook feature interactions, less accurate.\n",
    "Wrapper Approach: Uses a specific model to evaluate feature subsets based on predictive performance.\n",
    "Pros: Considers feature interactions, potentially more accurate.\n",
    "Cons: Computationally intensive, risk of overfitting.\n",
    "\n",
    "Ans 4:\n",
    "i. Feature Selection Process:\n",
    "\n",
    "Define objective (e.g., improve model performance).\n",
    "Choose a method (filter, wrapper, embedded).\n",
    "Evaluate feature subsets using chosen metric.\n",
    "Select subset with best performance.\n",
    "ii. Feature Extraction Principle: Transforming raw data into a more compact representation capturing essential information. Example: Principal Component Analysis (PCA) extracts orthogonal components maximizing variance.\n",
    "\n",
    "Widely Used Algorithms: PCA, Linear Discriminant Analysis (LDA), t-SNE.\n",
    "\n",
    "Ans 5:\n",
    "In text categorization, feature engineering might involve:\n",
    "\n",
    "Tokenization: Breaking text into words or n-grams.\n",
    "Text Cleaning: Removing stopwords, punctuation.\n",
    "Feature Encoding: Converting text to numerical format, e.g., TF-IDF, word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "970de2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.675\n"
     ]
    }
   ],
   "source": [
    "#Ans 6:\n",
    "#Cosine similarity measures the cosine of the angle between two vectors, indicating similarity in direction regardless of magnitude. \n",
    "import numpy as np\n",
    "\n",
    "# Given vectors\n",
    "A = np.array([2, 3, 2, 0, 2, 3, 3, 0, 1])\n",
    "B = np.array([2, 1, 0, 0, 3, 2, 1, 3, 1])\n",
    "\n",
    "# Calculate cosine similarity\n",
    "dot_product = np.dot(A, B)\n",
    "magnitude_A = np.linalg.norm(A)\n",
    "magnitude_B = np.linalg.norm(B)\n",
    "\n",
    "cosine_similarity = dot_product / (magnitude_A * magnitude_B)\n",
    "print(f\"Cosine Similarity: {cosine_similarity:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0be6f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Distance: 2\n",
      "Jaccard Index: 1.00\n"
     ]
    }
   ],
   "source": [
    "#Ans:7\n",
    "# Given binary strings\n",
    "str1 = '10001011'\n",
    "str2 = '11001111'\n",
    "\n",
    "# Calculate Hamming Distance\n",
    "hamming_distance = sum(b1 != b2 for b1, b2 in zip(str1, str2))\n",
    "print(f\"Hamming Distance: {hamming_distance}\")\n",
    "\n",
    "# Given sets for Jaccard Index\n",
    "set1 = set([1, 1, 0, 0, 1, 0, 1, 1])\n",
    "set2 = set([1, 1, 0, 0, 0, 1, 1, 1])\n",
    "\n",
    "# Calculate Jaccard Index\n",
    "intersection = len(set1.intersection(set2))\n",
    "union = len(set1.union(set2))\n",
    "jaccard_index = intersection / union\n",
    "print(f\"Jaccard Index: {jaccard_index:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1090c30",
   "metadata": {},
   "source": [
    "Ans 8:\n",
    "A high-dimensional dataset has a large number of features or dimensions, potentially exceeding the number of samples. Examples include image data with pixel values, genomic data with thousands of genes. Challenges include increased computational complexity, risk of overfitting, and difficulties in visualization. Solutions involve dimensionality reduction techniques like PCA or feature selection methods.\n",
    "\n",
    "Ans 9:\n",
    "a. PCA (Principal Component Analysis): A dimensionality reduction technique, not related to Personal Computer Analysis.\n",
    "b. Use of Vectors: Vectors represent data points in a multi-dimensional space, essential in machine learning algorithms.\n",
    "c. Embedded Technique: Feature selection method integrated within the model training process, e.g., Lasso regression.\n",
    "\n",
    "Ans 10:\n",
    "a. Sequential Backward Exclusion vs. Sequential Forward Selection:\n",
    "\n",
    "Backward Exclusion: Starts with all features and removes them sequentially.\n",
    "Forward Selection: Starts with no features and adds them sequentially based on performance.\n",
    "b. Filter vs. Wrapper Methods:\n",
    "Filter: Evaluates features independently.\n",
    "Wrapper: Evaluates feature subsets using specific models.\n",
    "c. SMC vs. Jaccard Coefficient:\n",
    "SMC: Similarity coefficient based on shared features.\n",
    "Jaccard: Measures similarity as ratio of shared to total features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63b07b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
