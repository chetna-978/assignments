{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac8dcb9",
   "metadata": {},
   "source": [
    "1. Explain One-Hot Encoding\n",
    "2. Explain Bag of Words\n",
    "3. Explain Bag of N-Grams\n",
    "4. Explain TF-IDF\n",
    "5. What is OOV problem?\n",
    "6. What are word embeddings?\n",
    "7. Explain Continuous bag of words (CBOW)\n",
    "8. Explain SkipGram\n",
    "9. Explain Glove Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2618ad4b",
   "metadata": {},
   "source": [
    "Ans 1: One-Hot Encoding\n",
    "\n",
    "One-Hot Encoding is a method used to convert categorical data variables into a form that can be provided to machine learning algorithms. It converts categorical variables into a binary vector where each category is represented by a binary digit.\n",
    "Example: Suppose we have categories ['red', 'blue', 'green']. One-hot encoding for 'blue' would be [0, 1, 0].\n",
    "\n",
    "Ans 2: Bag of Words (BoW)\n",
    "\n",
    "The Bag of Words model represents text data as a collection of words (or tokens) disregarding grammar and word order. It focuses on the occurrence of words in a document rather than their sequence.\n",
    "Example: For the sentence \"The cat sat on the mat\", the BoW representation might be {The: 2, cat: 1, sat: 1, on: 1, mat: 1}.\n",
    "\n",
    "Ans 3: Bag of N-Grams\n",
    "\n",
    "Similar to BoW, but instead of individual words, it considers sequences of n contiguous words (or tokens). It captures some context as it includes phrases of length n.\n",
    "Example: For the sentence \"The cat sat on the mat\", with n=2, the BoN representation might include \"The cat\", \"cat sat\", \"sat on\", \"on the\", \"the mat\".\n",
    "\n",
    "Ans 4: TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "\n",
    "TF-IDF is a numerical statistic used in text mining and information retrieval to measure the importance of a word in a document relative to a collection of documents (corpus).\n",
    "It weighs words by how often they appear in a document (TF) but also compensates for words that are frequently used across many documents (IDF).\n",
    "\n",
    "Ans 5: OOV (Out-of-Vocabulary) Problem\n",
    "\n",
    "The OOV problem arises when a word that appears in the test or evaluation data has not been seen during the training phase. This is common in real-world applications where new words or terminologies emerge over time.\n",
    "Solutions include using special tokens for unknown words or techniques like character-level embeddings.\n",
    "\n",
    "Ans 6: Word Embeddings\n",
    "\n",
    "Word embeddings are dense vector representations of words in a high-dimensional space. They capture semantic meanings and relationships between words. Words with similar meanings are closer in the embedding space.\n",
    "Examples include Word2Vec, GloVe, and FastText embeddings.\n",
    "\n",
    "Ans 7: Continuous Bag of Words (CBOW)\n",
    "\n",
    "CBOW is a model used to generate word embeddings by predicting a target word based on its context (surrounding words). It tries to predict a word given its neighboring words.\n",
    "Example: For the sentence \"The cat sat on the mat\", CBOW might predict \"sat\" given the context \"The cat on the\".\n",
    "\n",
    "Ans 8: SkipGram\n",
    "\n",
    "SkipGram is the inverse of CBOW. Instead of predicting a word given its context, SkipGram predicts the surrounding words given a target word. It's known for capturing rare words and phrases well.\n",
    "Using the same example as CBOW, SkipGram would predict \"The\", \"cat\", \"on\", and \"the\" given the target word \"sat\".\n",
    "\n",
    "Ans 9: GloVe Embeddings\n",
    "\n",
    "GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm for obtaining word embeddings. It considers global statistics of the entire corpus to create word vectors.\n",
    "Unlike Word2Vec's local context window, GloVe focuses on the co-occurrence statistics of words across the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e6edb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
