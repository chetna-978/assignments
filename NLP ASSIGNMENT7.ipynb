{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90b55d04",
   "metadata": {},
   "source": [
    "1. Explain the architecture of BERT\n",
    "2. Explain Masked Language Modeling (MLM)\n",
    "3. Explain Next Sentence Prediction (NSP)\n",
    "4. What is Matthews evaluation?\n",
    "5. What is Matthews Correlation Coefficient (MCC)?\n",
    "6. Explain Semantic Role Labeling\n",
    "7. Why Fine-tuning a BERT model takes less time than pretraining\n",
    "8. Recognizing Textual Entailment (RTE)\n",
    "9. Explain the decoder stack of GPT models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933bf99e",
   "metadata": {},
   "source": [
    "Ans 1: Architecture of BERT\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) is a transformer-based model designed for natural language processing tasks. Its architecture consists of multiple transformer encoder layers stacked on top of each other. Each encoder layer consists of a multi-head self-attention mechanism and position-wise feed-forward networks. BERT is trained using a masked language modeling objective, where random words are masked in the input, and the model is trained to predict these masked words based on their context. Additionally, BERT introduces a novel pre-training strategy called \"next sentence prediction,\" where the model is trained to predict whether two sentences follow each other in the original text.\n",
    "\n",
    "Ans 2: Masked Language Modeling (MLM)\n",
    "\n",
    "Masked Language Modeling (MLM) is a pre-training objective used in models like BERT. In MLM, a certain percentage of the input tokens are randomly masked (replaced with a [MASK] token), and the model is trained to predict these masked tokens based on the surrounding context. By learning to predict masked tokens, the model develops a deep understanding of the language's syntactic and semantic structures and can effectively capture the relationships between different words and phrases.\n",
    "\n",
    "Ans 3: Next Sentence Prediction (NSP)\n",
    "\n",
    "Next Sentence Prediction (NSP) is another pre-training objective used in models like BERT. In NSP, the model is presented with pairs of sentences and trained to predict whether the second sentence follows the first in the original text. By learning to predict the relationship between consecutive sentences, the model can capture the discourse structure and contextual dependencies between different parts of a text, enabling it to perform well on a wide range of downstream tasks.\n",
    "\n",
    "Ans 4: Matthews Evaluation\n",
    "\n",
    "Matthews evaluation is a method used to evaluate the performance of classification models, particularly in binary classification tasks. It computes the Matthews Correlation Coefficient (MCC), a metric that takes into account true positives, true negatives, false positives, and false negatives and provides a balanced measure of the classifier's quality, even when the classes are imbalanced.\n",
    "\n",
    "Ans 5: Matthews Correlation Coefficient (MCC)\n",
    "\n",
    "Matthews Correlation Coefficient (MCC) is a metric used to evaluate the performance of binary classification models. It ranges from -1 to +1, where +1 indicates perfect prediction, 0 indicates random prediction, and -1 indicates inverse prediction. MCC takes into account true positives, true negatives, false positives, and false negatives and provides a balanced measure of the classifier's quality, making it particularly useful for imbalanced datasets.\n",
    "\n",
    "Ans 6: Semantic Role Labeling\n",
    "\n",
    "Semantic Role Labeling (SRL) is a natural language processing task that involves identifying and classifying the roles of words or phrases in a sentence with respect to a predicate. For example, in the sentence \"She gave him a book,\" the roles of the words \"she,\" \"him,\" and \"a book\" with respect to the predicate \"gave\" can be labeled as agent, recipient, and theme, respectively. SRL is crucial for tasks like information extraction, question answering, and machine translation.\n",
    "\n",
    "Ans 7: Fine-tuning a BERT Model\n",
    "\n",
    "Fine-tuning a BERT model involves adapting a pre-trained BERT model to a specific downstream task, such as text classification or named entity recognition. Since BERT is already trained on a large corpus of text, fine-tuning typically requires less time and computational resources than training a model from scratch. By fine-tuning BERT on a task-specific dataset, the model can leverage its pre-trained knowledge and achieve high performance with relatively few training examples.\n",
    "\n",
    "Ans 8: Recognizing Textual Entailment (RTE)\n",
    "\n",
    "Recognizing Textual Entailment (RTE) is a natural language processing task that involves determining whether a given text (the hypothesis) logically follows from another text (the premise). For example, in the premise \"All men are mortal\" and the hypothesis \"Socrates is a man,\" the task is to recognize that the hypothesis logically follows from the premise. RTE is a fundamental task in natural language understanding and has applications in question answering, information retrieval, and dialogue systems.\n",
    "\n",
    "Ans 9: Decoder Stack of GPT Models\n",
    "\n",
    "The decoder stack of GPT (Generative Pre-trained Transformer) models consists of multiple transformer decoder layers stacked on top of each other. Each decoder layer comprises a multi-head self-attention mechanism, cross-attention mechanism (for tasks like language modeling), and position-wise feed-forward networks. During training, GPT models are trained to predict the next token in a sequence given its preceding context, and they can generate coherent and contextually relevant text across a wide range of tasks, such as text completion, summarization, and question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97a74a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
