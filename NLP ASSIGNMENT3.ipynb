{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b76068f",
   "metadata": {},
   "source": [
    "1. Explain the basic architecture of RNN cell.\n",
    "2. Explain Backpropagation through time (BPTT)\n",
    "3. Explain Vanishing and exploding gradients\n",
    "4. Explain Long short-term memory (LSTM)\n",
    "5. Explain Gated recurrent unit (GRU)\n",
    "6. Explain Peephole LSTM\n",
    "7. Bidirectional RNNs\n",
    "8. Explain the gates of LSTM with equations.\n",
    "9. Explain BiLSTM\n",
    "10. Explain BiGRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e13d55",
   "metadata": {},
   "source": [
    "Ans 1: Basic Architecture of RNN Cell\n",
    "\n",
    "The basic architecture of an RNN cell consists of:\n",
    "Input: Takes an input vector and an optional hidden state from the previous time step.\n",
    "Hidden State: Represents the memory of the RNN. It's a vector that captures information from previous inputs.\n",
    "Activation Function: Processes the combined input and hidden state.\n",
    "Output: Produces an output vector and passes a hidden state to the next time step.\n",
    "\n",
    "Ans 2: Backpropagation Through Time (BPTT)\n",
    "\n",
    "BPTT is an extension of the backpropagation algorithm used to train RNNs. It unfolds the network in time and computes gradients over all time steps. The idea is to treat the RNN as a deep network with shared weights and use standard backpropagation to update the weights based on the error at each time step.\n",
    "\n",
    "Ans 3: Vanishing and Exploding Gradients\n",
    "\n",
    "Vanishing Gradients: Occur when the gradients in the network become too small during backpropagation, causing the weights to update very slowly or not at all. This typically happens in deep networks and RNNs, especially when using certain activation functions like the sigmoid or tanh.\n",
    "\n",
    "Exploding Gradients: Occur when the gradients become too large, causing unstable updates and potentially leading to numerical overflow. This can be especially problematic in RNNs trained with BPTT.\n",
    "\n",
    "Ans 4: Long Short-Term Memory (LSTM)\n",
    "\n",
    "LSTM is a type of RNN cell designed to capture long-term dependencies in sequences. It has a more complex architecture with three gates (input, forget, and output) and a cell state to control the flow of information. The gates help regulate the information flow, allowing LSTMs to remember or forget information over long sequences.\n",
    "\n",
    "Ans 5: Gated Recurrent Unit (GRU)\n",
    "\n",
    "GRU is another variant of the basic RNN cell designed to address some of the limitations of the LSTM, with a simpler architecture. It has two gates (reset and update) and uses an update gate to control the flow of information, similar to the forget and input gates in LSTM.\n",
    "\n",
    "Ans 6: Peephole LSTM\n",
    "\n",
    "Peephole LSTM is an extension of the standard LSTM architecture that allows the gates to peek at the cell state directly, in addition to the hidden state. This provides the gates with more information to make better decisions about when to open or close, potentially improving the model's performance.\n",
    "\n",
    "Ans 7: Bidirectional RNNs\n",
    "\n",
    "Bidirectional RNNs process sequences in both forward and backward directions. This allows the model to capture information from both past and future contexts, making it useful for tasks that require a more comprehensive understanding of the input sequence.\n",
    "\n",
    "Ans 9: BiLSTM\n",
    "\n",
    "BiLSTM combines the forward and backward information by using two LSTMs: one processing the sequence from start to end and the other from end to start. The outputs from both LSTMs are concatenated at each time step, providing a richer representation of the input sequence that captures both past and future contexts.\n",
    "\n",
    "Ans 10: BiGRU\n",
    "\n",
    "BiGRU is similar to BiLSTM but uses GRU cells instead of LSTMs. It processes the input sequence in both forward and backward directions, combining the outputs to produce a more comprehensive representation of the sequence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0aac42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
